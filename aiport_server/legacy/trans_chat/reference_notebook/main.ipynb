{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python387jvsc74a57bd0ff34a1100df01d090f2aa8425e39efffac82a1f92e5efdbb81cd127e9b10227a",
   "display_name": "Python 3.8.7 64-bit ('.base_env': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Import Modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PyTorch version:[1.8.1+cu111].\ndevice:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from data_loader import generate_conversation_dataset\n",
    "from trans_model.torch_transformer import TransformerTransModel, generate_padding_mask, generate_square_subsequent_mask\n",
    "from datetime import datetime\n",
    "from utils import translate, save_checkpoint, load_checkpoint\n",
    "\n",
    "print(\"PyTorch version:[%s].\" % (torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:[%s].\" % (device))"
   ]
  },
  {
   "source": [
    "# Generate Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 5/4563 [00:00<01:52, 40.65it/s]Torkenizing and generating vocabs...\n",
      "100%|██████████| 4563/4563 [00:49<00:00, 91.84it/s]Spliting dataset...\n",
      "Indexing Senteces (Test set)...\n",
      "Indexing Senteces (Validation set)...\n",
      "Indexing Senteces (Train set)...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "random_seed = 0\n",
    "\n",
    "train_set, validation_set, test_set = generate_conversation_dataset(\"../raw_data/conversations.csv\", batch_size=batch_size, seed=random_seed, validation_ratio=0, test_ratio=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(tensor([[   2,    2],\n        [3674,  590],\n        [   9, 2661],\n        [  34,   67],\n        [3674, 3194],\n        [   9, 4179],\n        [  43,  231],\n        [3674,  627],\n        [1990,  893],\n        [2994, 4153],\n        [1442, 3237],\n        [2967, 2047],\n        [4103, 4103],\n        [  88, 2641],\n        [1990, 2563],\n        [2994,   13],\n        [  56, 2047],\n        [3674, 4103],\n        [1990,  627],\n        [3035,  744],\n        [1263, 3035],\n        [4103, 3579],\n        [2596, 3194],\n        [  13, 4179],\n        [ 947, 3035],\n        [1362, 1265],\n        [1423,  289],\n        [   9, 3035],\n        [2057, 2028],\n        [ 127, 1605],\n        [3434, 2985],\n        [2994, 1322],\n        [3428, 2994],\n        [4103, 2282],\n        [ 378, 2928],\n        [4023, 4103],\n        [   9,  893],\n        [2075, 1637],\n        [ 691, 3035],\n        [4103,  107],\n        [ 893, 2216],\n        [1990, 1090],\n        [4251, 3183],\n        [3587,  252],\n        [  13,   13],\n        [   3,    3]]), tensor([[   2,    2],\n        [ 121, 1983],\n        [1571, 3714],\n        [1905, 3378],\n        [2489,  119],\n        [3715, 1990],\n        [2808, 3419],\n        [  26,  121],\n        [3188,  485],\n        [ 263, 2138],\n        [3704, 3211],\n        [1904,   26],\n        [ 791,  589],\n        [1983, 1990],\n        [ 606,   18],\n        [3715, 1633],\n        [3331, 3772],\n        [1905, 3733],\n        [  28, 3378],\n        [1882,   28],\n        [2543, 1990],\n        [4109, 1034],\n        [  26, 2509],\n        [1990, 1833],\n        [  18, 4158],\n        [ 261, 3733],\n        [1905,   26],\n        [4060,  589],\n        [2824, 3211],\n        [2234, 2260],\n        [2828,  429],\n        [ 263,  121],\n        [ 246, 4020],\n        [1127, 3772],\n        [3122,  796],\n        [ 263, 4162],\n        [3244, 4128],\n        [  28, 2312],\n        [   3,   28],\n        [   1,    3]]))\n"
     ]
    }
   ],
   "source": [
    "for item in train_set:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "source": [
    "# Define Models and Hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "num_epochs = 32\n",
    "learning_rate = 1e-4\n",
    "\n",
    "src_vocab_size = len(train_set.kor2idx)\n",
    "trg_vocab_size = len(train_set.eng2idx)\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dim_feed_forward = 2048\n",
    "dropout_rate = 0.1\n",
    "padding_vocab = train_set.kor2idx[\"<pad>\"]\n",
    "\n",
    "model = TransformerTransModel(\n",
    "    src_vocab_size, trg_vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    dim_feed_forward=dim_feed_forward,\n",
    "    dropout=dropout_rate,\n",
    "    padding_vocab_index=padding_vocab\n",
    ").to(device)\n",
    "\n",
    "\n",
    "load_model = True\n",
    "save_model = True\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=padding_vocab)\n",
    "\n",
    "if load_model:\n",
    "    load_checkpoint(torch.load(\"./checkpoint/checkpoint_last.pth.tar\"), model, optimizer)"
   ]
  },
  {
   "source": [
    "# Test Translation\n",
    "\n",
    "학습이 안되었으므로 당연히 제대로 결과가 나오지 않는다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = translate(model, \"안녕하세요? 또 뵙는군요.\", train_set.kor2idx, train_set.eng2idx, train_set.idx2eng, device=device, max_length=16)\n",
    "print()\n",
    "print(\"Final Output: \")\n",
    "print(output)\n",
    "print(len(output))"
   ]
  },
  {
   "source": [
    "# Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Start Epoch {epoch}: {datetime.now()}\")\n",
    "    for index, batch in enumerate(train_set):\n",
    "        source = batch[0].to(device)\n",
    "        target = batch[1].to(device)        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(source, target[:-1, :])\n",
    "\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # if save_model:\n",
    "    #     checkpoint = {\n",
    "    #         \"state_dict\": model.state_dict(),\n",
    "    #         \"optimizer\": optimizer.state_dict(),\n",
    "    #     }\n",
    "    #     save_checkpoint(checkpoint, filename=f\"./checkpoint/checkpoint_epoch_{epoch}.pth.tar\")\n",
    "    \n",
    "    if epoch % 10 == 9:\n",
    "        print(f\"Loss({epoch} / {num_epochs}): {loss}\")\n",
    "        print(f\"End: {datetime.now()}\")\n",
    "        print(\"\\n=================================\\n\")\n",
    "\n",
    "\n",
    "if save_model:\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"kor2idx\": train_set.kor2idx\n",
    "    }\n",
    "    save_checkpoint(checkpoint, filename=\"./checkpoint/checkpoint_last.pth.tar\")"
   ]
  },
  {
   "source": [
    "# Check Result"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"오늘 수업은 뭔가요?\"\n",
    "output = translate(model, test_text, train_set.kor2idx, train_set.eng2idx, train_set.idx2eng, device=device, max_length=16)\n",
    "print()\n",
    "print(\"Input: \")\n",
    "print(test_text)\n",
    "print()\n",
    "print(\"Final Output: \")\n",
    "print(output)\n",
    "# running on entire test data takes a while"
   ]
  },
  {
   "source": [
    "# 해야 할 일\n",
    "\n",
    "Load가 이루어지면 vocab 사전도 함께 로드할 수 있도록 data_loader를 수정\n",
    "\n",
    "Save시 vocab이 유지될 수 있도록 수정\n",
    "\n",
    "BLEU Score 함수 작성 \n",
    "\n",
    "프론트엔드, 백엔드 작성 (새 프로젝트)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}